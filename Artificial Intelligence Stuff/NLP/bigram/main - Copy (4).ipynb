{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "def ngrams(input_list, n):\n",
    "    return zip(*[input_list[i:] for i in range(n)]) #convert tokens to a bigram model\n",
    "\n",
    "with open('wiki100_1.txt', 'r', encoding='utf-8') as myfile: #loading text file (corpus) to string data\n",
    "    data=myfile.read().replace('\\n', '')\n",
    "    \n",
    "words = nltk.word_tokenize(data) #tokenize texts to words as a list element\n",
    "\n",
    "bigrams = ngrams(words, 2) #convert tokens to a bigram model\n",
    "\n",
    "model = Counter(bigrams) #add frequency of occurence to each bigram\n",
    "#print(model)#.most_common(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict the next word after word: salah\n",
      "\n",
      "predicted word after word \"salah\" is \"satu\" with probability:  78 / 89\n"
     ]
    }
   ],
   "source": [
    "already = 0\n",
    "wword = input('predict the next word after word: ') #input the desired word to be predicted\n",
    "predicted_word = ''\n",
    "pword_counter = 0\n",
    "probs=0\n",
    "for k,v in model.most_common():\n",
    "    if(k[0]==wword):\n",
    "        pword_counter += v #counting the total occurence of the desired word\n",
    "for k,v in model.most_common():\n",
    "    if(k[0]==wword): #choosing the first element since the list is sorted descendingly, the higher in the list has bigger probability\n",
    "        if(already==0):\n",
    "            already=1\n",
    "            predicted_word=k[1] #assign the word after the desired word that was inputted\n",
    "            probs=v\n",
    "print('')\n",
    "print('predicted word after word \"'+wword+'\" is \"'+predicted_word+'\" with probability: ',probs,'/',pword_counter) #output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PERPLEXITY WITHOUT SMOOTHING\n",
      "Enter a sentence to be analyzed: bahwa yang adalah yang benar adalah namun tetapi bahwa\n",
      "['bahwa', 'yang', 'adalah', 'yang', 'benar', 'adalah', 'namun', 'tetapi', 'bahwa']\n",
      "bahwa yang\n",
      "both words exist in the bigram but never occured, not counted\n",
      "\n",
      "yang adalah\n",
      "both words exist in the bigram but never occured, not counted\n",
      "\n",
      "adalah yang\n",
      "probability:  6 / 707\n",
      "('adalah', 'yang') 6\n",
      "\n",
      "yang benar\n",
      "probability:  9 / 2845\n",
      "('yang', 'benar') 9\n",
      "\n",
      "benar adalah\n",
      "probability:  1 / 14\n",
      "('benar', 'adalah') 1\n",
      "\n",
      "adalah namun\n",
      "both words exist in the bigram but never occured, not counted\n",
      "\n",
      "namun tetapi\n",
      "both words exist in the bigram but never occured, not counted\n",
      "\n",
      "tetapi bahwa\n",
      "both words exist in the bigram but never occured, not counted\n",
      "\n",
      "1.917626574895214e-06\n",
      "perplexity is:  4.317659998077821\n"
     ]
    }
   ],
   "source": [
    "print('PERPLEXITY WITHOUT SMOOTHING')\n",
    "sentence = input('Enter a sentence to be analyzed: ') #input a sentence to be checked its perplexity\n",
    "sentence_map = nltk.word_tokenize(sentence) #convert the sentence to list of words\n",
    "print(sentence_map)\n",
    "prob=1\n",
    "i=0\n",
    "while i<len(sentence_map)-1: #iterate through list of words\n",
    "    print(sentence_map[i],sentence_map[i+1])\n",
    "    adaf=False #first word boolean\n",
    "    adas=False #sedcond word boolean\n",
    "    notbigram=True #in bigram boolean\n",
    "    fwordc=0   \n",
    "    for k,v in model.most_common(): #iterate to check if first or second word exists\n",
    "        if(k[0]==sentence_map[i]):\n",
    "            fwordc += v #counting the occurence of first word\n",
    "        if(k[0]!=sentence_map[i] and k[1]==sentence_map[i+1]):\n",
    "            adas=True #if second word exists\n",
    "        elif(k[0]==sentence_map[i] and k[1]!=sentence_map[i+1]):\n",
    "            adaf=True #if first word exists\n",
    "    if(adaf==True and adas==True):\n",
    "        for k,v in model.most_common(): #iterate through bigram to check if this bigram exists    \n",
    "            if(k[0]==sentence_map[i] and k[1]==sentence_map[i+1]): #if bigram occurence exists\n",
    "                prob *= v/(fwordc) #update/count probability\n",
    "                print('probability: ',v,'/',fwordc)\n",
    "                print(k,v)\n",
    "                notbigram=False #assign the fact that both words exist and did occur in bigram\n",
    "                break\n",
    "        if(notbigram==True):\n",
    "            print('both words exist in the bigram but never occured, not counted') #this happens if both words exists in bigram model, but never occur as bigram\n",
    "        \n",
    "    if(adaf==False and adas==True):\n",
    "        print(\"second word exist in the bigram but the first word doesn't\") #this happens if the first word does never exist in bigram model\n",
    "    elif(adaf==True and adas==False):\n",
    "        print(\"first word exist in the bigram but the second word doesn't\") #this happens if the second word does never exist in bigram model\n",
    "    elif(adaf==False and adas==False):\n",
    "        print(\"Both words don't exist in the bigram\")  #this happens if both words never exist in bigram model\n",
    "    print('')\n",
    "    i+=1\n",
    "print(prob) #counting the probability of the sentence\n",
    "perplexity = (1/prob)**(1/len(sentence_map)) #counting the perplexity\n",
    "print('perplexity is: ',perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PERPLEXITY WITH SMOOTHING\n",
      "Enter a sentence to be analyzed: bahwa yang adalah yang benar adalah namun tetapi bahwa\n",
      "['bahwa', 'yang', 'adalah', 'yang', 'benar', 'adalah', 'namun', 'tetapi', 'bahwa']\n",
      "\n",
      "|V|= 6\n",
      "bahwa yang\n",
      "probability before smoothing: not counted\n",
      "probability after smoothing: 1 / 128\n",
      "\n",
      "yang adalah\n",
      "probability before smoothing: not counted\n",
      "probability after smoothing: 1 / 2851\n",
      "\n",
      "adalah yang\n",
      "('adalah', 'yang') 6\n",
      "probability before smoothing:  6 / 707\n",
      "probability after smoothing:  7 / 713\n",
      "\n",
      "yang benar\n",
      "('yang', 'benar') 9\n",
      "probability before smoothing:  9 / 2845\n",
      "probability after smoothing:  10 / 2851\n",
      "\n",
      "benar adalah\n",
      "('benar', 'adalah') 1\n",
      "probability before smoothing:  1 / 14\n",
      "probability after smoothing:  2 / 20\n",
      "\n",
      "adalah namun\n",
      "probability before smoothing: not counted\n",
      "probability after smoothing: 1 / 713\n",
      "\n",
      "namun tetapi\n",
      "probability before smoothing: not counted\n",
      "probability after smoothing: 1 / 109\n",
      "\n",
      "tetapi bahwa\n",
      "probability before smoothing: not counted\n",
      "probability after smoothing: 1 / 57\n",
      "\n",
      "3.508354096247499e-11\n",
      "perplexity is:  14.509591165255056\n"
     ]
    }
   ],
   "source": [
    "print('PERPLEXITY WITH SMOOTHING')\n",
    "sentence = input('Enter a sentence to be analyzed: ') #input a sentence to be checked its perplexity\n",
    "sentence_map = nltk.word_tokenize(sentence) #conver the sentence to list of words\n",
    "print(sentence_map)\n",
    "voc = len(Counter(sentence_map)) #counting the number of words uniquely\n",
    "print('')\n",
    "print('|V|=',voc)\n",
    "prob=1\n",
    "i=0\n",
    "while i<len(sentence_map)-1: #iterate through list of words\n",
    "    print(sentence_map[i],sentence_map[i+1])\n",
    "    adaf=False #first word boolean\n",
    "    adas=False #sedcond word boolean\n",
    "    notbigram=True #in bigram boolean\n",
    "    fwordc=0\n",
    "    for k,v in model.most_common(): #iterate to check if first or second word exists\n",
    "        if(k[0]==sentence_map[i]):\n",
    "            fwordc += v #counting the occurence of first word\n",
    "        if(k[0]!=sentence_map[i] and k[1]==sentence_map[i+1]):\n",
    "            adas=True #if second word exists\n",
    "        elif(k[0]==sentence_map[i] and k[1]!=sentence_map[i+1]):\n",
    "            adaf=True #if first word exists\n",
    "    if(adaf==True and adas==True):\n",
    "        for k,v in model.most_common(): #iterate through bigram to check if this bigram exists           \n",
    "            if(k[0]==sentence_map[i] and k[1]==sentence_map[i+1]): #if bigram occurence exists\n",
    "                prob *= v+1/(fwordc+voc) #update/count probability\n",
    "                print(k,v)\n",
    "                print('probability before smoothing: ',v,'/',fwordc)\n",
    "                print('probability after smoothing: ',v+1,'/',fwordc+voc)\n",
    "                notbigram=False #assign the fact that both words exist and did occur in bigram\n",
    "                break\n",
    "        if(notbigram==True):\n",
    "            prob *= 1/(fwordc+voc) #since it's with smoothing technique, the unknown occurence is taken into account to be counted\n",
    "            print('probability before smoothing: not counted')\n",
    "            print('probability after smoothing:',1,'/',fwordc+voc)\n",
    "        \n",
    "    if(adaf==False and adas==True):\n",
    "        print(\"second word exist in the bigram but the first word doesn't\") #this happens if the first word does never exist in bigram model\n",
    "    elif(adaf==True and adas==False):\n",
    "        print(\"first word exist in the bigram but the second word doesn't\") #this happens if the second word does never exist in bigram model\n",
    "    elif(adaf==False and adas==False):\n",
    "        print(\"Both words don't exist in the bigram\")  #this happens if both words never exist in bigram model\n",
    "    print('')\n",
    "    i+=1\n",
    "print(prob) #counting the probability of the sentence\n",
    "perplexity = (1/prob)**(1/len(sentence_map)) #counting the perplexity\n",
    "print('perplexity is: ',perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
