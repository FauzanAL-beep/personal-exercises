{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "def ngrams(input_list, n):\n",
    "    return zip(*[input_list[i:] for i in range(n)]) #convert tokens to a bigram model\n",
    "\n",
    "with open('wiki100_1.txt', 'r', encoding='utf-8') as myfile: #loading text file (corpus) to string data\n",
    "    data=myfile.read().replace('\\n', '')\n",
    "    \n",
    "words = nltk.word_tokenize(data) #tokenize texts to words as a list element\n",
    "\n",
    "bigrams = ngrams(words, 2) #convert tokens to a bigram model\n",
    "\n",
    "model = Counter(bigrams) #add frequency of occurence to each bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "already = 0\n",
    "wword = input('predict the next word after word: ') #input the desired word to be predicted\n",
    "predicted_word = ''\n",
    "pword_counter = 0\n",
    "probs=0\n",
    "for k,v in model.most_common():\n",
    "    if(k[0]==wword):\n",
    "        pword_counter += v #counting the total occurence of the desired word\n",
    "for k,v in model.most_common():\n",
    "    if(k[0]==wword): #choosing the first element since the list is sorted descendingly, the higher in the list has bigger probability\n",
    "        if(already==0):\n",
    "            already=1\n",
    "            predicted_word=k[1] #assign the word after the desired word that was inputted\n",
    "            probs=v\n",
    "print('')\n",
    "print('predicted word after word \"'+wword+'\" is \"'+predicted_word+'\" with probability: ',probs,'/',pword_counter) #output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('PERPLEXITY WITH SMOOTHING')\n",
    "sentence = input('Enter a sentence to be analyzed: ') #input a sentence to be checked its perplexity\n",
    "sentence_map = nltk.word_tokenize(sentence) #conver the sentence to list of words\n",
    "print(sentence_map)\n",
    "voc = len(Counter(sentence_map)) #counting the number of words uniquely\n",
    "print('')\n",
    "print('|V|=',voc)\n",
    "prob=1\n",
    "i=0\n",
    "while i<len(sentence_map)-1: #iterate through list of words\n",
    "    print(sentence_map[i],sentence_map[i+1])\n",
    "    adaf=False #first word boolean\n",
    "    adas=False #sedcond word boolean\n",
    "    notbigram=True #in bigram boolean\n",
    "    fwordc=0\n",
    "    for k,v in model.most_common(): #iterate to check if first or second word exists\n",
    "        if(k[0]==sentence_map[i]):\n",
    "            fwordc += v #counting the occurence of first word\n",
    "        if(k[0]!=sentence_map[i] and k[1]==sentence_map[i+1]):\n",
    "            adas=True #if second word exists\n",
    "        elif(k[0]==sentence_map[i] and k[1]!=sentence_map[i+1]):\n",
    "            adaf=True #if first word exists\n",
    "    if(adaf==True and adas==True):\n",
    "        for k,v in model.most_common(): #iterate through bigram to check if this bigram exists           \n",
    "            if(k[0]==sentence_map[i] and k[1]==sentence_map[i+1]): #if bigram occurence exists\n",
    "                prob *= v+1/(fwordc+voc) #update/count probability\n",
    "                print(k,v)\n",
    "                print('probability before smoothing: ',v,'/',fwordc)\n",
    "                print('probability after smoothing: ',v+1,'/',fwordc+voc)\n",
    "                notbigram=False #assign the fact that both words exist and did occur in bigram\n",
    "                break\n",
    "        if(notbigram==True):\n",
    "            prob *= 1/(fwordc+voc) #since it's with smoothing technique, the unknown occurence is taken into account to be counted\n",
    "            print('probability before smoothing: not counted')\n",
    "            print('probability after smoothing:',1,'/',fwordc+voc)\n",
    "        \n",
    "    if(adaf==False and adas==True):\n",
    "        print(\"second word exist in the bigram but the first word doesn't\") #this happens if the first word does never exist in bigram model\n",
    "    elif(adaf==True and adas==False):\n",
    "        print(\"first word exist in the bigram but the second word doesn't\") #this happens if the second word does never exist in bigram model\n",
    "    elif(adaf==False and adas==False):\n",
    "        print(\"Both words don't exist in the bigram\")  #this happens if both words never exist in bigram model\n",
    "    print('')\n",
    "    i+=1\n",
    "print('Sentence probability: ',prob) #counting the probability of the sentence\n",
    "perplexity = (1/prob)**(1/len(sentence_map)) #counting the perplexity\n",
    "print('Perplexity is: ',perplexity)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
