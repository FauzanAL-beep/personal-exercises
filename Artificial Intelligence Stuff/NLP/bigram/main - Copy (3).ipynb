{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "def ngrams(input_list, n):\n",
    "    return zip(*[input_list[i:] for i in range(n)])\n",
    "\n",
    "with open('wiki100_1.txt', 'r', encoding='utf-8') as myfile: #loading text file (corpus) to string data\n",
    "    data=myfile.read().replace('\\n', '')\n",
    "    \n",
    "words = nltk.word_tokenize(data) #tokenize texts to words as a list element\n",
    "\n",
    "bigrams = ngrams(words, 2) #convert tokens to a bigram model\n",
    "\n",
    "model = Counter(bigrams) #add frequency of occurence to each bigram\n",
    "#print(model)#.most_common(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict the next word after word: salah\n",
      "\n",
      "predicted word after word \"salah\" is \"satu\"\n"
     ]
    }
   ],
   "source": [
    "already = 0\n",
    "wword = input('predict the next word after word: ') #input the desired word to be predicted\n",
    "predicted_word = ''\n",
    "pword_counter = 0\n",
    "for k,v in model.most_common():\n",
    "    if(k[0]==wword):\n",
    "        pword_counter += v #counting the total occurence of the desired word\n",
    "for k,v in model.most_common():\n",
    "    if(k[0]==wword): #choosing the first element since the list is sorted descendingly, the higher in the list has bigger probability\n",
    "        if(already==0):\n",
    "            already=1\n",
    "            predicted_word=k[1] #assign the word after the desired word that was inputted\n",
    "        #print(k,\"with probability:\",v/pword_counter) #this is to print the model from the word input\n",
    "print('')\n",
    "print('predicted word after word \"'+wword+'\" is \"'+predicted_word+'\"') #output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PERPLEXITY WITHOUT SMOOTHING\n",
      "Enter a sentence to be analyzed: akan tetapi sehingga yang terjadi adalah kebersamaan kebersamaan sekerbanm namun karena kebersamaan dengan\n",
      "['akan', 'tetapi', 'sehingga', 'yang', 'terjadi', 'adalah', 'kebersamaan', 'kebersamaan', 'sekerbanm', 'namun', 'karena', 'kebersamaan', 'dengan']\n",
      "akan tetapi\n",
      "('akan', 'tetapi') 11\n",
      "ada dua2nya\n",
      "\n",
      "tetapi sehingga\n",
      "ada first ada second tapi gak ada bigram\n",
      "\n",
      "sehingga yang\n",
      "('sehingga', 'yang') 1\n",
      "ada dua2nya\n",
      "\n",
      "yang terjadi\n",
      "('yang', 'terjadi') 46\n",
      "ada dua2nya\n",
      "\n",
      "terjadi adalah\n",
      "('terjadi', 'adalah') 3\n",
      "ada dua2nya\n",
      "\n",
      "adalah kebersamaan\n",
      "ada first\n",
      "\n",
      "kebersamaan kebersamaan\n",
      "tidak ada dua2nya\n",
      "\n",
      "kebersamaan sekerbanm\n",
      "tidak ada dua2nya\n",
      "\n",
      "sekerbanm namun\n",
      "ada second\n",
      "\n",
      "namun karena\n",
      "('namun', 'karena') 3\n",
      "ada dua2nya\n",
      "\n",
      "karena kebersamaan\n",
      "ada first\n",
      "\n",
      "kebersamaan dengan\n",
      "ada second\n",
      "\n",
      "1.8709440327282167e-09\n",
      "perplexity is:  4.6922369891028985\n"
     ]
    }
   ],
   "source": [
    "print('PERPLEXITY WITHOUT SMOOTHING')\n",
    "sentence = input('Enter a sentence to be analyzed: ') #input a sentence to be checked its perplexity\n",
    "sentence_map = nltk.word_tokenize(sentence) #conver the sentence to list of words\n",
    "print(sentence_map)\n",
    "prob=1\n",
    "i=0\n",
    "# adaf=False\n",
    "# adas=False\n",
    "# notbigram=True\n",
    "while i<len(sentence_map)-1:\n",
    "    print(sentence_map[i],sentence_map[i+1])\n",
    "    adaf=False\n",
    "    adas=False\n",
    "    notbigram=True\n",
    "    fwordc=0\n",
    "    for k,v in model.most_common():\n",
    "        if(k[0]==sentence_map[i]):\n",
    "            fwordc += 1\n",
    "    for k,v in model.most_common():\n",
    "        #if(k[0]==sentence_map[i] or k[1]==sentence_map[i+1]):\n",
    "        \n",
    "        if(k[0]!=sentence_map[i] and k[1]==sentence_map[i+1]):\n",
    "            #print(k,v)\n",
    "            adas=True\n",
    "        elif(k[0]==sentence_map[i] and k[1]!=sentence_map[i+1]):\n",
    "            #print(k,v)\n",
    "            adaf=True\n",
    "    if(adaf==True and adas==True):\n",
    "        for k,v in model.most_common():        \n",
    "            if(k[0]==sentence_map[i] and k[1]==sentence_map[i+1]):\n",
    "                #print('probability: ',v,'/',fwordc)\n",
    "                prob *= v/(fwordc)\n",
    "                print(k,v)\n",
    "                print('ada dua2nya')\n",
    "                notbigram=False\n",
    "                break\n",
    "        if(notbigram==True):\n",
    "            print('ada first ada second tapi gak ada bigram')\n",
    "        \n",
    "        \n",
    "    if(adaf==False and adas==True):\n",
    "        print('ada second')\n",
    "    elif(adaf==True and adas==False):\n",
    "        print('ada first')\n",
    "    elif(adaf==False and adas==False):\n",
    "        print('tidak ada dua2nya')\n",
    "    print('')\n",
    "    i+=1\n",
    "print(prob)\n",
    "perplexity = (1/prob)**(1/len(sentence_map))\n",
    "print('perplexity is: ',perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = input('Enter a sentence to be analyzed: ') #input a sentence to be checked its perplexity\n",
    "sentence_map = nltk.word_tokenize(sentence) #conver the sentence to list of words\n",
    "print(sentence_map)\n",
    "voc=0\n",
    "i=0\n",
    "prob=1\n",
    "while i<len(sentence_map)-1:\n",
    "    ada = 0\n",
    "    for k,v in model.most_common():\n",
    "        if(k[0]==sentence_map[i] and k[1]==sentence_map[i+1]):\n",
    "            ada=1\n",
    "            break\n",
    "    if(ada==0):\n",
    "        voc+=1\n",
    "    i+=1\n",
    "print('voc: ',voc)\n",
    "i=0\n",
    "while i<len(sentence_map)-1:\n",
    "    print(sentence_map[i],sentence_map[i+1])\n",
    "    fwordc=0\n",
    "    fw=False\n",
    "    bw=False\n",
    "    for k,v in model.most_common():\n",
    "        if(k[0]==sentence_map[i]):\n",
    "            fwordc += 1\n",
    "#             print('nih: ',k[0])\n",
    "#             print('nih: ',sentence_map[i])\n",
    "    for k,v in model.most_common():\n",
    "        #if(k[0]==sentence_map[i] or k[1]==sentence_map[i+1]):\n",
    "        if(k[0]==sentence_map[i] and k[1]==sentence_map[i+1]):\n",
    "            print('before smoothing: ',v,'/',fwordc)\n",
    "            prob *= v+1/(fwordc+voc)\n",
    "            print('after smoothing: ',v+1,'/',fwordc+voc)\n",
    "            print(k,v)\n",
    "            fw=True\n",
    "            bw=True\n",
    "        if(k[0]!=sentence_map[i] and k[1]==sentence_map[i+1]):\n",
    "            bw=True\n",
    "        if(k[0]==sentence_map[i] and k[1]!=sentence_map[i+1]):\n",
    "            fw=True\n",
    "            \n",
    "#         elif(k[0]!=sentence_map[i] and k[1]==sentence_map[i+1]):\n",
    "#             prob *= 1/(fwordc+voc)\n",
    "#             print('first word not found, after smoothing: ',1,'/',fwordc+1)\n",
    "    if(fw==True and bw==False):\n",
    "        print('second word was not found, after smoothing (belom): ',1,'/',fwordc+voc)\n",
    "        #prob *= 1/(fwordc+voc)#gtw\n",
    "    if(fw==False and bw==True):\n",
    "        #print(fwordc)\n",
    "        print(\"first word wasn't not found, after smoothing: \",1,'/',fwordc+voc)\n",
    "        prob *= 1/(fwordc+voc)\n",
    "    if(fw==False and bw==False):\n",
    "        #print(fwordc)\n",
    "        print(\"both words weren't not found, after smoothing  (belom): \",1,'/',fwordc+voc)\n",
    "        #prob *= 1/(fwordc+voc)\n",
    "        \n",
    "    print('')\n",
    "    i+=1\n",
    "print(prob)\n",
    "perplexity = (1/prob)**(1/len(sentence_map))\n",
    "print('perplexity is: ',perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
